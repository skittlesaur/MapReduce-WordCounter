Today, the Internet is a big place where huge numbers of information are uploaded every day. Large amounts of information are undeniably being accumulated as a result of our social, mobile, and virtual world. KiloByte, MegaByte, and GigaByte are terms that are often used nowadays. Nonetheless, because of the great advances in computers, networking, and data in general, we are not far from the day when terminology like PetaByte, ExaByte, and ZettaByte will be more prevalent than those stated above. 
This massive flow of data generated by users' tasks, referred to as the Big Data Era, makes it challenging for enterprises to preserve and extract essential information in order to provide efficient and user-friendly services. In the Big Data Era, storing massive amounts of data is no longer the most difficult task. Companies already keep massive amounts of data. For example, Facebook is able to keep activity data dating back to 2005 in its backend (Roush, 2014). Researchers have found themselves involved in the most difficult operations such as quick data lookup, modification, and data updating due to the nature of this data, localization, and extensive languages. To achieve the scalability and performance requirements, efficient parallel and concurrent algorithms and implementation approaches are required.
 The MapReduce pattern, inspired by the programming language Lisp, allowed end users to define various types of parallel operations using Map and Reduce functions, without having to worry about the parallelism aspects